# ğŸ‘‹ Hi, I'm Vamsi Krishna  
### Data Engineer | Cloud Data Pipelines | ELT/ETL | Automation | Analytics Engineering

Iâ€™m a Data Engineer specializing in building scalable **cloud data platforms**, automating **ELT/ETL pipelines**, and enabling analytics teams through well-modeled, reliable, production-grade data systems. I enjoy solving complex data problems using clean architecture, efficient transformations, and thoughtful automation.

---

## ğŸš€ About Me
- ğŸ› ï¸ 5+ years of experience building data solutions in **healthcare, finance, and telecom**  
- â˜ï¸ Hands-on with **Azure, AWS, and GCP** for warehouse, compute, and orchestration  
- ğŸ§± Strong in **data modeling**, **pipeline optimization**, and **distributed processing**  
- âš™ï¸ Passionate about **dbt**, **Airflow**, **Databricks**, and **Snowflake**  
- ğŸ“Š Experienced with **claims data**, analytics layers, and KPI/metric design  
- ğŸ¤ I collaborate closely with analysts, ML engineers, and business partners  

---

## ğŸ§° Tech Stack & Tools

### **Languages**
- Python  
- SQL  
- PySpark  
- Shell Scripting  

### **Cloud & Data Warehouses**
- Snowflake  
- Azure Data Factory  
- Azure Synapse  
- Databricks  
- AWS Glue  
- BigQuery  

### **Orchestration / Workflow**
- Apache Airflow  
- dbt  
- CI/CD (GitHub Actions, Azure DevOps)  

### **Streaming & Messaging**
- Kafka  
- EventHub / PubSub  

### **Data Modeling & Architecture**
- Star/Snowflake schema  
- Lakehouse design  
- Medallion architecture  
- Data Quality frameworks  

---

## ğŸ“‚ Featured Projects

### ğŸ”¹ **1. End-to-End ELT Pipeline with dbt + Snowflake**
**Tech:** dbt, Snowflake, Airflow, Python  
- Built a production-grade ELT pipeline with modularized dbt models (staging â†’ core â†’ marts).  
- Implemented tests, documentation, macros, and performance optimizations.  
- Automated scheduled runs using Airflow with lineage visibility.

---

### ğŸ”¹ **2. Streaming Ingestion Pipeline using Kafka + Databricks**
**Tech:** Kafka, Databricks, PySpark, Delta Lake  
- Processed high-volume event data in near real-time.  
- Implemented schema enforcement, checkpointing, and incremental writes.  
- Enabled downstream analysts to access curated Delta tables.

---

### ğŸ”¹ **3. Automated Data Quality Framework**
**Tech:** Python, dbt tests, Great Expectations 
- Built reusable rules for null checks, primary keys, uniqueness, and referential integrity.  
- Integrated rules into the CI/CD pipeline to block bad deployments.

---

### ğŸ”¹ **4. Healthcare Claims Analytics Model**
**Tech:** SQL, Python, Airflow, Snowflake  
- Designed a claims data model for eligibility, pharmacy, CPT/ICD grouping, and utilization metrics.  
- Improved reporting accuracy and refresh time for analytics teams.

---

## ğŸ“ˆ What I'm Currently Learning
- Advanced **data observability** techniques  
- **Large-scale streaming patterns** with Kafka / Spark  
- Optimizing **warehouse compute costs** in Snowflake and BigQuery  
- ML feature engineering pipelines (for MLOps alignment)

---

## ğŸ“« Connect With Me
- **LinkedIn:** https://www.linkedin.com/in/vamsikrishnapaladugu
- **GitHub:** https://github.com/vamsi-krishna-paladugu  
- **Email:** vkrishnapl12@gmail.com

---

### â­ If you like my projects, feel free to star the repo or connect with me â€” always happy to collaborate!
